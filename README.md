# 3D Bin Packing with Deep Reinforcement Learning

Production-ready implementation of **TAP-Net** (Transformer-based Actor-critic Packing Network) for solving the 3D bin packing problem using Deep Reinforcement Learning and PPO.

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

## Overview

This project implements the state-of-the-art algorithm from the paper:

> **"A deep reinforcement learning approach for the 3D bin packing problem with online item arrival"**
> Deng et al., 2023, *Expert Systems with Applications*
> Paper: `reference/1-s2.0-S0957417422021716-main.pdf`

### Key Features

- **Transformer-based Architecture**: Multi-head self-attention for spatial reasoning
- **Height Map Representation**: Efficient 2D grid for O(1) collision detection
- **PPO Training**: Stable policy gradient with GAE advantage estimation
- **Action Masking**: Heuristic-guided filtering for faster convergence
- **Interactive Visualization**: 3D Plotly visualizations with animation support
- **Checkpoint Management**: Automatic save/resume with best model tracking
- **TensorBoard Integration**: Real-time training monitoring
- **Production-Ready**: Comprehensive error handling, logging, and testing

### Performance

Based on paper results:
- **Space Utilization**: 80-90% on standard benchmarks
- **Training Time**: 10-20 hours on GPU for 1M timesteps
- **Convergence**: Visible improvement within 100K timesteps

## Installation

### Prerequisites

- Python 3.8+
- CUDA-capable GPU (optional, but recommended)

### Setup

```bash
# Clone the repository
git clone <repository-url>
cd transformer-3d-packing-rl

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Verify Installation

```bash
python -c "import torch; print(f'PyTorch: {torch.__version__}')"
python -c "import gymnasium; print(f'Gymnasium: {gymnasium.__version__}')"
```

## Quick Start

### Training

```bash
# Train with default configuration
python train.py

# Train on GPU
python train.py --device cuda

# Train with custom timesteps
python train.py --total-timesteps 2000000 --device cuda

# Resume from checkpoint
python train.py --resume
```

### Monitoring

```bash
# Launch TensorBoard (in a separate terminal)
tensorboard --logdir logs/

# Open browser to http://localhost:6006
```

### Evaluation

```bash
# Evaluate trained model
python evaluate.py --checkpoint checkpoints/best.pt

# Evaluate with 3D visualization
python evaluate.py --checkpoint checkpoints/best.pt --visualize --save-html

# Evaluate 50 episodes
python evaluate.py --checkpoint checkpoints/best.pt --n-episodes 50
```

## Project Structure

```
transformer-3d-packing-rl/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ environment/        # 3D packing environment
â”‚   â”‚   â”œâ”€â”€ container.py    # Container with height map
â”‚   â”‚   â”œâ”€â”€ item.py         # Items with 6 rotations
â”‚   â”‚   â”œâ”€â”€ action_mask.py  # Heuristic action masking
â”‚   â”‚   â””â”€â”€ packing_env.py  # Gymnasium environment
â”‚   â”œâ”€â”€ models/             # Neural network models
â”‚   â”‚   â”œâ”€â”€ tap_net.py      # Main TAP-Net model
â”‚   â”‚   â”œâ”€â”€ actor.py        # Transformer actor network
â”‚   â”‚   â””â”€â”€ critic.py       # Value network
â”‚   â”œâ”€â”€ training/           # Training infrastructure
â”‚   â”‚   â”œâ”€â”€ ppo_trainer.py  # PPO algorithm
â”‚   â”‚   â”œâ”€â”€ replay_buffer.py # Rollout buffer with GAE
â”‚   â”‚   â””â”€â”€ checkpoint_manager.py # Checkpoint management
â”‚   â”œâ”€â”€ visualization/      # Visualization tools
â”‚   â”‚   â”œâ”€â”€ plotly_3d.py    # 3D interactive plots
â”‚   â”‚   â””â”€â”€ training_plots.py # Training curves
â”‚   â””â”€â”€ utils/              # Utilities
â”‚       â”œâ”€â”€ config.py       # Configuration management
â”‚       â”œâ”€â”€ logger.py       # Logging setup
â”‚       â””â”€â”€ metrics.py      # Evaluation metrics
â”œâ”€â”€ config/
â”‚   â””â”€â”€ default.yaml        # Default hyperparameters
â”œâ”€â”€ reference/              # Paper reference code
â”œâ”€â”€ train.py                # Main training script
â”œâ”€â”€ evaluate.py             # Evaluation script
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ CLAUDE.md               # Development guide
â””â”€â”€ README.md               # This file
```

## Architecture

### Environment

**Container with Height Map**
- 2D grid tracking surface heights at each position
- O(1) collision detection and placement validation
- Metrics: space utilization, compactness, stability

**Action Space**
- Discrete: position (grid_x, grid_y) Ã— rotation (0-5)
- Total: grid_size Ã— grid_size Ã— 6 actions (e.g., 10Ã—10Ã—6 = 600)
- Reduced via heuristic action masking

**State Representation**
- Height map: (grid_size, grid_size) normalized heights
- Item features: (5,) [length, width, height, volume, weight]
- Metadata: (3,) [packed_ratio, num_packed, remaining]

**Reward Function**
```python
reward = 0.6 * utilization + 0.2 * compactness + 0.2 * stability
```

### Model (TAP-Net)

**Actor Network**
1. Height map CNN encoder â†’ spatial features
2. Item MLP encoder â†’ item embedding
3. Transformer encoder (multi-head self-attention)
4. Action decoder â†’ probability distribution

**Critic Network**
- Similar architecture to actor
- Outputs scalar state value V(s)

**Training (PPO)**
- Proximal Policy Optimization with clipped objective
- Generalized Advantage Estimation (GAE)
- Multiple epochs per batch of experience

## Configuration

Edit `config/default.yaml` to customize:

```yaml
environment:
  container_size: [10.0, 10.0, 10.0]
  grid_size: 10
  max_items: 50
  reward_type: "hybrid"

model:
  d_model: 256
  nhead: 8
  num_layers: 4

training:
  total_timesteps: 1_000_000
  learning_rate: 0.0003
  gamma: 0.99
  gae_lambda: 0.95
  clip_epsilon: 0.2
```

## Advanced Usage

### Custom Configuration

```bash
# Create custom config
cp config/default.yaml config/my_config.yaml

# Edit config file
# ... modify hyperparameters ...

# Train with custom config
python train.py --config config/my_config.yaml
```

### Programmatic Usage

```python
from src.environment.packing_env import PackingEnv
from src.models.tap_net import TAPNet
from src.training.ppo_trainer import PPOTrainer

# Create environment
env = PackingEnv(
    container_size=(10, 10, 10),
    grid_size=10,
    max_items=50,
)

# Create model
model = TAPNet(
    grid_size=10,
    d_model=256,
    nhead=8,
    num_layers=4,
)

# Create trainer
trainer = PPOTrainer(model=model, env=env)

# Train
trainer.train(total_timesteps=100000)
```

### Visualization Example

```python
from src.visualization.plotly_3d import PackingVisualizer

# Create visualizer
viz = PackingVisualizer()

# Visualize container
fig = viz.visualize_container(container)

# Save as HTML
viz.save_html("packing_result.html")

# Or show interactively
viz.show()
```

## Benchmarking

Compare with baseline methods:

```bash
# Train TAP-Net
python train.py --experiment-name tap_net

# Compare results
python evaluate.py --checkpoint checkpoints/best.pt --n-episodes 100
```

## Troubleshooting

### Common Issues

**Out of Memory**
```bash
# Reduce batch size
python train.py --config config/small_batch.yaml
```

**Training Not Converging**
- Reduce learning rate in config
- Increase buffer_size for more stable gradients
- Check TensorBoard for NaN losses

**Low Utilization**
- Verify action masking is enabled
- Increase training timesteps
- Adjust reward weights in `packing_env.py`

### Debug Mode

```python
# Enable anomaly detection in train.py
import torch
torch.autograd.set_detect_anomaly(True)
```

<!-- ## Development

### Running Tests

```bash
# Install test dependencies
pip install pytest pytest-cov

# Run tests
pytest tests/

# With coverage
pytest --cov=src tests/
```

### Code Formatting

```bash
# Install formatters
pip install black flake8

# Format code
black src/ train.py evaluate.py

# Check style
flake8 src/ train.py evaluate.py
``` -->

## Citation

If you use this code, please cite the original paper:

```bibtex
@article{deng2023deep,
  title={A deep reinforcement learning approach for the 3D bin packing problem with online item arrival},
  author={Deng, Hang and Chen, Ying and Zeng, Chongyan and others},
  journal={Expert Systems with Applications},
  volume={210},
  pages={118476},
  year={2023},
  publisher={Elsevier}
}
```
<!-- 
## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- Paper authors for the TAP-Net algorithm
- OpenAI for Gym/Gymnasium framework
- Plotly team for visualization library
- PyTorch team for the deep learning framework

## Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes with tests
4. Submit a pull request

## Contact

For questions or issues:
- Open an issue on GitHub
- Check `CLAUDE.md` for development guidance

## Roadmap

- [ ] Multi-container support
- [ ] Online training with continual learning
- [ ] Model quantization for deployment
- [ ] ONNX export for inference
- [ ] Real-world item datasets
- [ ] Comparison with other RL algorithms (DQN, A3C)
- [ ] Web interface for interactive packing

--- -->

**Happy Packing!** ðŸ“¦
